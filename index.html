<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
	<title>Self-Supervised Monocular Scene Decomposition and Depth Estimation</title>
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
	<meta property="og:image" content="Path to my teaser.jpg"/>
	<meta property="og:title" content="Self-Supervised Monocular Scene Decomposition and Depth Estimation" />
	<meta property="og:description" content="Monocular Depth Estimation" />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary_large_image" />
    <meta property="twitter:title"         content="Self-Supervised Monocular Scene Decomposition and Depth Estimation" />
    <meta property="twitter:description"   content="Monocular Depth Estimation" />
    <meta property="twitter:image"         content="Path to my teaser.jpg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        Self-Supervised Monocular Scene Decomposition and Depth Estimation
    </div>

    <div class="venue">
        In 3DV 2021
    </div>

    <br><br>

    <div class="author">
        <a href="https://sadrasafa.github.io" target="_blank">Sadra Safadoust</a>
    </div>
    <div class="author">
        <a href="https://mysite.ku.edu.tr/fguney" target="_blank">Fatma Güney</a>
    </div>
    <br><br>
    <div class="affiliation"><a href="https://ai.ku.edu.tr" target="_blank"> KUIS AI Center, Koç University</a></div>

    <br><br>
    <span class="link-block">
      <a href="https://arxiv.org/pdf/2110.11275.pdf" class="external-link button is-normal is-rounded is-dark" style="background-color:#2a2a2a;">
        <span class="icon">
          <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg>
        </span>
    <span>Paper</span>
      </a>
    </span>
    <span class="link-block">
      <a href="https://arxiv.org/abs/2110.11275" class="external-link button is-normal is-rounded is-dark" style="background-color:#2a2a2a;">
        <span class="icon">
          <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 384" data-fa-i2svg=""><path fill="currentColor" d="m 119.65,351.996 c -5.84961,0.13477 -11.1943,-3.29883 -13.502,-8.67578 -2.19629,-5.27051 -0.61914,-8.9668 4.19727,-15.8652 7.05469,-10.3799 78.8242,-96.5625 78.8242,-96.5625 l -15.8828,-14.8633 c -13.3809,-13.3779 -13.9561,-31.333 -1.50977,-43.7754 l 18.4922,-17.6133 -51.5977,-63.377 c -4.00586,-4.26758 -6.48535,-11.7559 -4.24805,-17.1309 2.27832,-5.53613 7.69727,-9.12891 13.6836,-9.07031 3.83398,0.0957 7.43262,1.87402 9.83789,4.86133 l 61.3691,57.0566 94.5762,-90.0703 c 3.19434,-3.08398 7.44336,-4.83984 11.8828,-4.91016 1.60449,0.0039 3.19824,0.245117 4.73242,0.714844 5.7793,1.80566 10.249,6.41797 11.8711,12.252 1.2998,5.47363 -0.27637,11.2334 -4.18164,15.2832 l -83.0859,100.011996 14.8789,13.834 c 11.0957,10.0029 11.1543,27.3906 0.12695,37.4688 l -16.2949,15.6309 56.2559,66.4434 0.0742,0.0859 0.0664,0.0899 c 5.02734,6.53125 7.43164,11.5615 4.83984,17.9395 -3.13379,5.96875 -8.69727,10.29 -15.2559,11.8516 -0.67676,0.0908 -1.35938,0.13574 -2.04297,0.13671 l -0.004,-0.0117 c -4.42871,-0.27051 -8.61035,-2.13086 -11.7754,-5.24023 l -0.13086,-0.10743 -0.12304,-0.11132 -65.207,-59.127 -89.8965,86.2402 c 0,0 -5.33398,6.47754 -10.9707,6.61133 z m 178.104,-33.041 c 0.47852,-0.002 0.95703,-0.0332 1.43164,-0.0957 4.84277,-1.33301 8.95605,-4.54004 11.4316,-8.91016 1.44922,-3.5625 1.00293,-6.45703 -4.19727,-13.2148 l -56.0586,-66.2188 -26.375,25.3027 64.9551,58.9062 c 2.33887,2.41504 5.46484,3.91113 8.8125,4.2168 v 0.0137 z M 192.436,227.402 334.446,57.244 c 2.78711,-3.48926 4.5293,-6.97949 3.3418,-10.9121 -1.16113,-4.30273 -4.43457,-7.71777 -8.68555,-9.05664 -1.08105,-0.333984 -2.20508,-0.503906 -3.33594,-0.505859 -3.20801,0.07617 -6.26953,1.35645 -8.57617,3.58789 l -142.033,135.284999 c -11.1719,11.1719 -9.48242,26.0195 1.39258,36.8945 z"></path></svg><i class="fas fa-file-pdf"></i>
        </span>
        <span>Arxiv</span>
      </a>
    </span>
    <span class="link-block">
      <a class="external-link button is-normal is-rounded is-dark" style="background-color:#2a2a2a;">
        <span class="icon">
            <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
        </span>
        <span>Code (coming)</span>
      </a>
    </span>
    <span class="link-block">
      <a href="resources/poster.png" class="external-link button is-normal is-rounded is-dark" style="background-color:#2a2a2a;">
        <span class="icon">
          <svg class="svg-inline--fa fa-palette fa-w-16" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="palette" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M204.3 5C104.9 24.4 24.8 104.3 5.2 203.4c-37 187 131.7 326.4 258.8 306.7 41.2-6.4 61.4-54.6 42.5-91.7-23.1-45.4 9.9-98.4 60.9-98.4h79.7c35.8 0 64.8-29.6 64.9-65.3C511.5 97.1 368.1-26.9 204.3 5zM96 320c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm32-128c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128-64c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32zm128 64c-17.7 0-32-14.3-32-32s14.3-32 32-32 32 14.3 32 32-14.3 32-32 32z"></path></svg>
        </span>
        <span>Poster</span>
      </a>
    </span>
    <span class="link-block">	
      <a href="https://slideslive.com/38972173/selfsupervised-monocular-scene-decomposition-and-depth-estimation" class="external-link button is-normal is-rounded is-dark" style="background-color:#2a2a2a;">
        <span class="icon">
          <!-- <svg xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 512 512">! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc.<path d="M448 32H361.9l-1 1-127 127h92.1l1-1L453.8 32.3c-1.9-.2-3.8-.3-5.8-.3zm64 128V96c0-15.1-5.3-29.1-14-40l-104 104H512zM294.1 32H201.9l-1 1L73.9 160h92.1l1-1 127-127zM64 32C28.7 32 0 60.7 0 96v64H6.1l1-1 127-127H64zM512 192H0V416c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V192z"/></svg> -->
          <svg class="svg-inline--fa fa-clapperboard fa-w-18" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="clapperboard" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M448 32H361.9l-1 1-127 127h92.1l1-1L453.8 32.3c-1.9-.2-3.8-.3-5.8-.3zm64 128V96c0-15.1-5.3-29.1-14-40l-104 104H512zM294.1 32H201.9l-1 1L73.9 160h92.1l1-1 127-127zM64 32C28.7 32 0 60.7 0 96v64H6.1l1-1 127-127H64zM512 192H0V416c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V192z"></path></svg>
        </span>
        <span>Presentation</span>
      </a>
      </span>
    <!-- <div class="links" align=center><a>[Code coming]</a></div>
    <div class="links" align=center><a href="resources/poster.png" target="_blank">[Poster]</a></div>
    <div class="links" align=center><a href="https://slideslive.com/38972173" target="_blank">[Presentation]</a></div> -->
    <br><br>

    <hr>
    <img style="width: 90%;" src="./resources/overview.png"
     alt="Method overview figure"/>
     <!--<p style="width: 90%;">
       <b>Left:</b> The depth network outputs depth estimate <b>D̂</b><sub><i>t</i></sub> for input <b>I</b><sub><i>t</i></sub> . <b>Right:</b> Given <b>D̂</b><sub><i>t</i></sub> and two consecutive frames <b>I</b><sub><i>s</i></sub> and <b>I</b><sub><i>t</i></sub> , the shared encoder maps the input frames and the depth estimate to a common representation shared by the following two decoders. The mask decoder produces the same resolution <i>K</i> masks {<b>M</b><sub><i>1</i></sub>, &hellip;, <b>M</b><sub><i>K</i></sub> } with skip connections between the corresponding layers of the encoder. The pose decoder takes the same encoded representation and converts it into rigid transformations {<b>T</b><sub><i>1</i></sub>, &hellip; , <b>T</b><sub><i>K</i></sub> } corresponding to the masks. Given the set of rigid transformations and corresponding masks, we transform a 3D point as a convex combination of estimated transformations weighted according to estimated masks. The number of components, <i>K</i>, is a hyper-parameter of our model.
     </p>
     -->
     <p style="width:90%; line-height: 1.5;">
       Self-supervised monocular depth estimation approaches
       either ignore independently moving objects in the scene or
       need a separate segmentation step to identify them. We propose
       <b>MonoDepthSeg</b> to jointly estimate depth and segment
       moving objects from monocular video without using any
       ground-truth labels. We decompose the scene into a fixed
       number of components where each component corresponds
       to a region on the image with its own transformation matrix
       representing its motion. We estimate both the mask and the
       motion of each component efficiently with a shared encoder.
       We evaluate our method on three driving datasets and show
       that our model clearly improves depth estimation while decomposing
       the scene into separately moving components.
     </p>
     <hr>
     <table style="width: 50%;" align=center>
       <tr>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/zoomed/img.png' alt="Input Image"/><span class="tooltiptextleft">Input <br> Image </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/zoomed/masks.png' alt="Our Scene Decomposition"/> <span class="tooltiptextright">Our Scene Decomposition </span></div></td>
       </tr>
       <tr>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/zoomed/monodepth2_disp.png' alt="Monodepth2's Depth Estimation"/><span class="tooltiptextleft">Monodepth2's Depth Estimation </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/zoomed/our_disp.png' alt="Our Depth Estimation"/><span class="tooltiptextright">Our Depth Estimation </span></div> </td>
       </tr>
     </table>
     <p style="width: 80%; line-height: 1.5;">
      Monocular depth estimation methods assume a static scene by relying
       on the ego-motion to explain the scene and fail in foreground
      regions with independently moving objects (<b>bottom-left</b>: <a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Godard_Digging_Into_Self-Supervised_Monocular_Depth_Estimation_ICCV_2019_paper.html"> Monodepth2</a>).
      By decomposing the scene into a set of components, we estimate a
      separate rigid transformation for each component, representing its
      motion. This improves the results in regions with moving objects
      (<b>bottom-right</b>) while simultaneously recovering a decomposition
      of the scene, mostly corresponding to moving regions (<b>top-right</b>).
     </p>
     <hr>
     <div style="width: 100%" align="center">
     <div class="inner-container">
       <div class="video-overlay">
          <div class="empty-tl tooltip-video">
            <span class="tooltiptextleft">Input <br> Image </span>
          </div>
          <div class="empty-tr tooltip-video">
            <span class="tooltiptextright">Our Scene Decomposition </span>
          </div>
          <div class="empty-bl tooltip-video">
            <span class="tooltiptextleft">Monodepth2's Depth Estimation </span>
          </div>
          <div class="empty-br tooltip-video">
            <span class="tooltiptextright">Our Depth Estimation </span>
          </div>
        </div>
       <video  autoplay loop muted loop>
         <source src="./resources/video_1024.mp4" type="video/mp4">
             Your browser does not support the video tag.
      </video>
    </div>
    </div>
    <br><br>
    <a href= "https://github.com/KUIS-AI/monodepthseg/blob/gh-pages/resources/video_1024.mp4?raw=true" target="_blank">Download Video <img style="width: 32px;" src='./resources/dl.png' alt="Download Button"/></a>
    <br><br>
    <!-- <video  autoplay loop muted style="height: 100px">
      <source src="./resources/video.mp4" type="video/mp4">
          Your browser does not support the video tag.
   </video> -->
    <hr>

     <!--
     <table style="width: 90%;" align=center>
       <tr>
         <td> <div class="tooltip"><img style="width: 100%;" heigth=100  src='./resources/qualitative/qual1_img.png' alt="Input Image"/><span class="tooltiptexttop">Input <br> Image </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual1_monodepth2.png' alt="Monodepth2's Depth Estimation"/> <span class="tooltiptexttop">Monodepth2's Depth Estimation </span></div></td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual1_our.png' alt="Our Depth Estimation"/><span class="tooltiptexttop">Our Depth Estimation </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual1_mask.png' alt="Our Scene Decomposition"/><span class="tooltiptexttop">Our Scene Decomposition </span></div> </td>
       </tr>
       <tr>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual2_img.png' alt="Input Image"/><span class="tooltiptexttop">Input <br> Image </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual2_monodepth2.png' alt="Monodepth2's Depth Estimation"/> <span class="tooltiptexttop">Monodepth2's Depth Estimation </span></div></td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual2_our.png' alt="Our Depth Estimation"/><span class="tooltiptexttop"> Our Depth Estimation</span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual2_mask.png' alt="Our Scene Decomposition"/><span class="tooltiptexttop">Our Scene Decomposition </span></div> </td>
       </tr>
       <tr>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual3_img.png' alt="Input Image"/><span class="tooltiptexttop">Input <br> Image </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual3_monodepth2.png' alt="Monodepth2's Depth Estimation"/> <span class="tooltiptexttop">Monodepth2's Depth Estimation</span></div></td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual3_our.png' alt="Our Depth Estimation"/><span class="tooltiptexttop">Our Depth Estimation</span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual3_mask.png' alt="Our Scene Decomposition"/><span class="tooltiptexttop">Our Scene Decomposition</span></div> </td>
       </tr>
       <tr>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual4_img.png' alt="Input Image"/><span class="tooltiptexttop">Input <br> Image </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual4_monodepth2.png' alt="Monodepth2's Depth Estimation"/> <span class="tooltiptexttop">Monodepth2's Depth Estimation</span></div></td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual4_our.png' alt="Our Depth Estimation"/><span class="tooltiptexttop">Our Depth Estimation </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual4_mask.png' alt="Our Scene Decomposition"/><span class="tooltiptexttop">Our Scene Decomposition </span></div> </td>
       </tr>
       <tr>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual5_img.png' alt="Input Image"/><span class="tooltiptexttop">Input <br> Image </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual5_monodepth2.png' alt="Monodepth2's Depth Estimation"/> <span class="tooltiptexttop">Monodepth2's Depth Estimation</span></div></td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual5_our.png' alt="Our Depth Estimation"/><span class="tooltiptexttop">Our Depth Estimation</span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual5_mask.png' alt="Our Scene Decomposition"/><span class="tooltiptexttop">Our Scene Decomposition</span></div> </td>
       </tr>
       <tr>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual6_img.png' alt="Input Image"/><span class="tooltiptexttop">Input <br> Image </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual6_monodepth2.png' alt="Monodepth2's Depth Estimation"/> <span class="tooltiptexttop">Monodepth2's Depth Estimation</span></div></td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual6_our.png' alt="Our Depth Estimation"/><span class="tooltiptexttop">Our Depth Estimation </span></div> </td>
         <td> <div class="tooltip"><img style="width: 100%;" src='./resources/qualitative/qual6_mask.png' alt="Our Scene Decomposition"/><span class="tooltiptexttop">Our Scene Decomposition </span></div> </td>
       </tr>
     </table> -->
     <h1>Paper</h1>
     <div class="paper-thumbnail">
        <a href="https://arxiv.org/abs/2110.11275">
            <img class="layered-paper-big" width="100%" src="./resources/paper.jpg" alt="Paper Thumbnail"/>
        </a>
    </div>
    <br>
    <div class="paper-info"style="width: 70%;">
        <h3 >Self-Supervised Monocular Scene Decomposition and Depth Estimation</h3>
        <p style="text-align: left;">Sadra Safadoust and Fatma Güney</p>
        <p style="text-align: left;">In 3DV, 2021.</p>
        <pre><code>@INPROCEEDINGS{monodepthseg,
  author={Safadoust, Sadra and Güney, Fatma},
  booktitle={International Conference on 3D Vision (3DV)},
  title={Self-Supervised Monocular Scene Decomposition and Depth Estimation},
  year={2021},
  pages={627-636}}
          </code></pre>
    </div>
    <hr>
    <h1>Acknowledgements</h1>
<p style="
    width: 70%;
    padding-bottom: 10%;
"><img src="./resources/eu_flag.jpg" align="right" valign="middle" vspace="0" hspace="66" style="
    width: 16%;
    margin-top: -2%;
">This project has received funding from KUIS AI Center, TÜBİTAK (118C256), EU Horizon 2020 under Marie Skłodowska-Curie grant (898466).</p>




</body>

</html>
